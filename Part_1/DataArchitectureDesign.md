# Data Architecture Design

![alt text](https://github.com/medicus/CCTExercise/main/ArchitectureDesign.jpg?raw=true)

## Ingestion of Client Data
Due to the continuous ingestion from client sites, the first step in the data pipeline I have chosen is to use Kinesis. Kinesis is best used when doing real-time streaming from applications and the ordering is important. Kinesis also allows for data to be read again in the same order up to 7 days later, which could be useful for certain use cases. One downside with using Kinesis is that you have to provision the shards ahead of time which could cause issues if experiencing an unexpectedly large amount of data. 

Another option that I considered was to use SQS in combination with Lambda. The advantage with SQS is that it scales automatically and can redeliver any failed messages. However, due to messages being processed one at a time, SQS introduced some delay. SQS would trigger the Lambda to do any processing needed and place the records into the Raw S3 bucket. 

In terms of cost, when dealing with large amounts of data SQS can be more expensive than Kinesis due to charging per message. Conversely, Kinesis can be expensive when dealing with small workloads because pricing is based upon provisioned shards rather than number of messages. I chose to include Kinesis in my diagram because it is the better choice if it can be properly provisioned based on expected quantities of data.

## Raw S3 Bucket Organization
In order to comply with the client’s strict data isolation requirement, I chose to create an S3 bucket per client. For this Raw S3 bucket, the client has provided CSV and JSON files from their on-premises sources. Due to this, I made the assumption that the continuous data coming from the client would also be in CSV and JSON files. 

Within the S3 bucket, we would partition by task_id. The field ‘task_id’ would be generated by kinesis when it is placed into the bucket. Once that specific task has been picked up by Glue and processed into the next step, a lambda could be created to archive that file and place it into an ‘Archive’ folder within the Raw bucket. If any point in the process fails, the lambda could also put that into a ‘Failed’ folder, and this would allow for quick access to any files that failed transformation. 

## Glue Transformation
For this step, Glue would be used to normalize all the data from the client and convert the CSV and JSON into Parquet files. Glue would output any logs to Cloudwatch and could also output an event to EventBridge for any errors. The EventBridge events could be used to send SNS alerts or to trigger the lambda that would put the data into the ‘Failed’ folder within the Raw bucket as described in the previous step.

This step converts the CSV and JSON into Parquet files in order to optimize processing. Parquet files allow for faster query performance by Glue and Redshift. Parquet files are also efficiently compressed and are optimized for cloud storage. Another advantage to Parquet files is that they allow for schema evolution over time. In terms of cost advantages, Parquet also produces reduced costs due to compressed storage and efficient processing that optimizes read and writes. 

Glue is best optimized for batch processing due to the long start up times, alternatively a lambda could be used in place of Glue for lightweight processing that needs to occur.
Processed S3 Bucket Organization
Similarly to the Raw S3 buckets, we would have a processed bucket per agency to comply with the data isolation requirement. This bucket would be partitioned and structured the same as the Raw bucket with the task_id being the main separation. This bucket would also have a ‘Failed’ and a ‘Archive’ folder for ease of troubleshooting. 

## Lake Formation & Glue Transformation
Lake Formation could be used between the S3 bucket and Glue to make it easier for Glue to process all the data between each client’s S3 bucket while ensuring data stays isolated. Lake Formation would also be used in combination with Glue Data Catalog to ensure the schema and metadata stays up to date. 

The glue job between the Processed S3 bucket and Redshift would be used for any complex transformations that would need to occur to get the data ready for analysis. 
Storage of Analytics Ready Data in Redshift
In order to focus on the reliability and scalability requirements, I chose to store the analytics ready data in Redshift. Redshift allows for consistent high performance for BI tools. Redshift also directly connects with Quicksight which I have chosen for the next step in order to comply with the self-service business intelligence requirement. Redshift is also optimized for analytical queries which is a core focus for the client.

DynamoDB is a good alternative to Redshift. DynamoDB excels in high volume transactional workloads. Depending how often and how much data the client sends, DynamoDB could be a more efficient source to store the analytical data. However, DynamoDB does not directly connect with Quicksight, which would cause another step to be needed to allow the data to be ingested into Quicksight.

## Self Service Business Intelligence with Quicksight
I chose Quicksight to be the self service business intelligence tool because it provides seamless integration with AWS tools including Redshift, S3, and Athena. Quicksight also creates interactive dashboards for analytical purposes and can be embedded within a website. Quicksight has built-in Machine Learning insights that can be used to detect anomalies. Quicksight provides integration with IAM and encryption to comply with security best practices. A dataset could be created per agency in order to enforce data isolation. This tool also has a very cost-effective model and only charges based on users actively interacting with the dashboard. Quicksight does have issues performing with large datasets, but with proper optimizations the issues can be limited. Datasets do provide the option to create calculated fields, but Glue may be a better option for doing any complex calculations to reduce load times in the dashboards.

## Monitoring & Reporting Capabilities (Cloudwatch)
Cloudwatch can be used throughout the solution to help with monitoring and reporting capabilities. In the design diagram, I connected it with Glue because Glue directly outputs all logs to Cloudwatch including output logs and error logs. These logs are immensely helpful in troubleshooting. Cloudwatch can also be configured to have custom metrics that can have alerts set up to trigger specific actions. These actions can include a connection to SNS which can send text or email messages.

Cloudwatch also has anomaly reporting capabilities that use machine learning to identify patterns. Cloudwatch is an important tool when used with ECS and can help display related metrics such as CPU utilization. Cloudwatch logs can also be exported and stored in S3 for compliance. 
